{"cells":[{"metadata":{"id":"9wHysZIKKQN2"},"cell_type":"markdown","source":"# Get data from google drive","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/databardl/bargraphs\"))\n\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######## Gpu Check ###########\nimport tensorflow as tf \ndevice_name = tf.test.gpu_device_name() \nif device_name != '/device:GPU:0':  \n    raise SystemError('GPU device not found') \nprint('Found GPU at: {}'.format(device_name)) ","execution_count":null,"outputs":[]},{"metadata":{"id":"wfwZXT1lft12","outputId":"014d0687-2e7b-4d30-999b-c77c0e80616b","trusted":true},"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive/')","execution_count":null,"outputs":[]},{"metadata":{"id":"5-rTLeDnKU0Y"},"cell_type":"markdown","source":"# Get data from zip file","execution_count":null},{"metadata":{"id":"i8scIGtzRSEy","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# import zipfile\n# zip_ref = zipfile.ZipFile('../input/data/natural_images', 'r')\n# zip_ref.extractall()\n# zip_ref.close()","execution_count":null,"outputs":[]},{"metadata":{"id":"8U4_HBQVlsY5","outputId":"a5e65331-d4ee-434c-f1b5-ad06f2bb85df","trusted":true},"cell_type":"code","source":"from os import listdir\nimg_dir = '../input/databardl/bargraphs'\ndata_list = listdir(img_dir)\ndata_list","execution_count":null,"outputs":[]},{"metadata":{"id":"YnEjN_mXHlY7"},"cell_type":"markdown","source":"# Resnet","execution_count":null},{"metadata":{"id":"rO5FH3KNEzK6"},"cell_type":"markdown","source":"## Image process and variable declare","execution_count":null},{"metadata":{"id":"Ln3kUv-E0lF2","outputId":"8d8c5e84-68f3-45d7-8e2a-f1495b71bfca","trusted":true},"cell_type":"code","source":"from tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model ,load_model\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout\nfrom tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport numpy as np\nimport tensorflow as tf\n\n\nDATASET_PATH  = img_dir\nIMAGE_SIZE    = (400, 400)\nNUM_CLASSES   = len(data_list)\nBATCH_SIZE    = 25  \nFREEZE_LAYERS = 16  \nNUM_EPOCHS    = 50\nLEARNING_RATE = 5e-5\nDROP_OUT = .5\nchkpoint_model_loc = '5'\n\n\ntrain_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n                                   rotation_range=0,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   shear_range=0.25,\n                                   zoom_range=0.1,\n                                   channel_shift_range = 20,\n                                   horizontal_flip = False ,\n                                   vertical_flip = False ,\n                                   validation_split = 0.2,\n                                   fill_mode='constant')\n\n# test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n#                                    fill_mode='constant')\n\ntrain_batches = train_datagen.flow_from_directory(DATASET_PATH,\n                                                  target_size=IMAGE_SIZE,\n                                                  shuffle=True,\n                                                  batch_size=BATCH_SIZE,\n                                                  subset = \"training\"\n                                                  )\n\nvalid_batches = train_datagen.flow_from_directory(DATASET_PATH,\n                                                  target_size=IMAGE_SIZE,\n                                                  shuffle=True,\n                                                  batch_size=BATCH_SIZE,\n                                                  subset = \"validation\"\n                                                  )\nclass_dictionary = train_batches.class_indices\nclass_dictionary\n","execution_count":null,"outputs":[]},{"metadata":{"id":"6pIlWAyG7dD5","outputId":"1b1e452a-c1cb-455e-948a-ab6103b8f250","trusted":true},"cell_type":"code","source":"len(chkpoint_model_loc)","execution_count":null,"outputs":[]},{"metadata":{"id":"c9PqefdOE7fG"},"cell_type":"markdown","source":"## Layer modification","execution_count":null},{"metadata":{"id":"2hmgE7kypVYI","trusted":true},"cell_type":"code","source":"# build our classifier model based on pre-trained InceptionResNetV2:\n# 1. we don't include the top (fully connected) layers of InceptionResNetV2\n# 2. we add a DropOut layer followed by a Dense (fully connected)\n#    layer which generates softmax class score for each class\n# 3. we compile the final model using an Adam optimizer, with a\n#    low learning rate (since we are 'fine-tuning')\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nif len(chkpoint_model_loc) < 5:\n#############main portion for InceptionResnetV2 only##############\n  net = InceptionResNetV2(include_top=False,\n                          weights='imagenet',\n                          input_tensor=None,\n                          input_shape=(299,299,3))\n\n  ############## Run this block if include top = False ######\n  x = net.output\n  x = GlobalAveragePooling2D()(x)\n#   x = Flatten()(x)\n#   x = Dropout(DROP_OUT)(x)\n###############   End of block ##################\n\n############## Run this block if include top = True ######\n#   '''\n#   This block keeps avg_pool (GlobalAveragePooling2) layer and removes only prediction layer.\n#   Then replace inceptionResnetv2's predictionlayer with a drpout layer and and a softmax layer. \n#   To change the class class size of 1000 to 8 we need to replace their pediction layer with our softmax layer.\n#   '''\n#   avg_pool = net.layers[-2]\n#   prediction = net.layers[-1]\n\n#   #create drpout layer\n#   drp1 = Dropout(DROP_OUT)\n#   x = drp1(avg_pool.output)\n###############   End of block ##################\n\n\n\n  output_layer = Dense(NUM_CLASSES, activation='softmax', name='softmax')(x)\n  net_final = Model(inputs=net.input, outputs=output_layer)\n\n  for layer in net_final.layers[:FREEZE_LAYERS]:\n      layer.trainable = False\n\n  for layer in net_final.layers[FREEZE_LAYERS:]:\n      layer.trainable = True\n\n  net_final.compile(optimizer=Adam(lr=LEARNING_RATE),\n                    loss='categorical_crossentropy', metrics=['accuracy'])\n\n#   print(net_final.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import applications\nimport numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.utils import class_weight, shuffle\n\nfrom keras import applications\nfrom keras import optimizers\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential, Model, load_model\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint\nbase_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(400,400,3))\n\nadd_model = Sequential()\nadd_model.add(Flatten(input_shape=base_model.output_shape[1:]))\nadd_model.add(Dense(256, activation='relu'))\nadd_model.add(Dense(4, activation='softmax'))\n\nmodel = Model(inputs=base_model.input, outputs=add_model(base_model.output))\nmodel.compile(loss='binary_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n              metrics=['accuracy'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"Tfz8JKiR_ACT","outputId":"7d7b5606-319e-47d4-d9a2-02ae866e2b94","trusted":true},"cell_type":"code","source":"print(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"id":"KGPG7yvxFGco"},"cell_type":"markdown","source":"## Checkpoint declare","execution_count":null},{"metadata":{"id":"ce0o-7Ug-0OA","trusted":true},"cell_type":"code","source":"#CHECKPOINT\n# filepath = \"../weights-improvement-EPOCH_{epoch:02d}-ACC_{acc:.2f}-VALIDATION_{val_acc:.2f}.hdf5\"\n# checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n# callbacks_list = [checkpoint]","execution_count":null,"outputs":[]},{"metadata":{"id":"jCqNOyOlFJ7A"},"cell_type":"markdown","source":"# load pretrained model","execution_count":null},{"metadata":{"id":"s3cPK7wV8CgX","outputId":"761150fb-3b69-4439-b5a5-e4096addde84","trusted":true},"cell_type":"code","source":"######## if running pre trained model is running #######\n# if len(chkpoint_model_loc) > 5:\n#   net_final = load_model(chkpoint_model_loc)\n#   net_final.evaluate_generator(valid_batches,\n#                                steps = np.ceil(len(valid_batches) / BATCH_SIZE),\n#                                verbose = 1\n#                                )\n","execution_count":null,"outputs":[]},{"metadata":{"id":"UX-a9RrnFBwU"},"cell_type":"markdown","source":"## Evaluation of loaded model","execution_count":null},{"metadata":{"id":"jfyN6LDkADPf","outputId":"4863a6e5-b280-492c-fa97-4b811513e42d","trusted":true},"cell_type":"code","source":"if len(chkpoint_model_loc) > 5:\n    \n    model.evaluate_generator(valid_batches,\n                               steps = np.ceil(len(valid_batches) / BATCH_SIZE),\n                               verbose = 1\n                               )\n","execution_count":null,"outputs":[]},{"metadata":{"id":"2oyHxZgsEba1"},"cell_type":"markdown","source":"# DropOut add","execution_count":null},{"metadata":{"id":"YAkjf9C6XOA3","trusted":true},"cell_type":"code","source":"#Adding Dropout layer to a pre trained model\ndrpout_needed = False\n\nif drpout_needed :\n  flt = net_final.layers[-2]\n  prediction = net_final.layers[-1]\n\n  #create drpout layer\n  drp1 = Dropout(DROP_OUT)\n  x = drp1(flt.output)\n\n  predictors = prediction(x)\n  net_final = Model(inputs=net_final.input, outputs=predictors)","execution_count":null,"outputs":[]},{"metadata":{"id":"LkHOPXB9Efp9"},"cell_type":"markdown","source":"# Summary","execution_count":null},{"metadata":{"id":"f98KQtBB8737","trusted":true},"cell_type":"code","source":"print(net_final.summary())","execution_count":null,"outputs":[]},{"metadata":{"id":"QTwRV01qqjKK","trusted":true},"cell_type":"code","source":"# !pip install tensorflow-gpu==2.0.0-alpha0\n# import tensorflow as tf\n# print(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit_generator(\n    train_datagen.flow(x_train, y_train, batch_size=batch_size),\n    steps_per_epoch = np.ceil(len(train_batches) / BATCH_SIZE),\n    epochs= 20,\n    validation_data = valid_batches,\n    callbacks=[ModelCheckpoint('VGG16-transferlearning.model', monitor='val_acc')]\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"Xgjx7vALEjdX"},"cell_type":"markdown","source":"# Run Training","execution_count":null},{"metadata":{"id":"_VgchRpXphB5","outputId":"87350b0f-30a2-44f3-a151-c42bae0d8d8a","trusted":true},"cell_type":"code","source":"\n#FIT MODEL\nmodel.fit_generator(train_batches,\n                        steps_per_epoch = np.ceil(len(train_batches) / BATCH_SIZE),\n                        validation_data = valid_batches,\n                        validation_steps = np.ceil(len(valid_batches) / BATCH_SIZE),\n                        epochs = NUM_EPOCHS,\n#                       callbacks=[ModelCheckpoint('VGG16-transferlearning.model', monitor='val_acc')] \n                       )\n","execution_count":null,"outputs":[]},{"metadata":{"id":"tl_xy-BmwvRI","trusted":true},"cell_type":"code","source":"# # save trained weights\n# # net_final.save(WEIGHTS_FINAL)\n# x = net_final.evaluate_generator(valid_batches,\n#                            steps = np.ceil(len(valid_batches) / BATCH_SIZE),\n#                            use_multiprocessing = True,\n#                            verbose = 1\n#                            )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# incorrects = np.nonzero(net_final.predict(valid_batches).reshape((-1,)) != valid_batches)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model3=model.save(\"bargraph_classifier_50epoch_vgg16.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model=tf.keras.models.load_model(\"../input/barplot-classification/bargraph_classifier_vgg16.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from PIL import Image\nimport pandas as pd\nfrom tensorflow.keras.preprocessing import image\nimport matplotlib.pyplot as plt\n\nIMAGE_SIZE    = (400, 400)\n# print(data_list)\nprint(\"Class name to class id map\\n\",class_dictionary)\n\n#test_image = image.load_img((img_dir + \"/motorbike/motorbike_0011.jpg\"),target_size =IMAGE_SIZE )\ntest_image = image.load_img(\"../input/supertest/stacked-bar-example-1.png\",target_size =IMAGE_SIZE )\ntest_image = image.img_to_array(test_image)\n\nplt.imshow(test_image/255.)\n\n# test_image = np.expand_dims(test_image , axis = 0)\ntest_image = test_image.reshape((1, test_image.shape[0], test_image.shape[1], test_image.shape[2]))\ntest_image = preprocess_input(test_image)\n\nprediction = model.predict(test_image)\ndf = pd.DataFrame({'pred':prediction[0]})\n# print(prediction[0])\ndf = df.sort_values(by='pred', ascending=False, na_position='first')\nprint(df)\n\nfor x in data_list:\n    if class_dictionary[x] == (df[df == df.iloc[0]].index[0]):\n        print(\"Class prediction = \", x)\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** pascale voc dataset\n**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"https://github.com/makatx/YOLO_ResNet/blob/master/Vehicle%20Detection.ipynb\nhttps://github.com/makatx/YOLO_ResNet/blob/master/Dataset%20Exploration.ipynb\n\n\nhttps://www.kaggle.com/zaraks/pascal-voc-2007/kernels\nhttps://www.kaggle.com/huanghanchina/pascal-voc-2012/kernels","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# import sys\n# import numpy as np\n# import cv2\n# import utils\n# import matplotlib.pyplot as plt\n\n\n# pred = net_final.predict(test_image)\n\n# bboxes = utils.get_boxes(pred[0], cutoff=0.1)\n# bboxes = utils.nonmax_suppression(bboxes, iou_cutoff = 0.05)\n# draw = utils.draw_boxes(img, bboxes, color=(0, 0, 255), thick=3, draw_dot=True, radius=3)\n# draw = draw.astype(np.uint8)\n\n# plt.imshow(draw[...,::-1])\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # from PIL import Image\n# import pandas as pd\n# from tensorflow.keras.preprocessing import image\n# import matplotlib.pyplot as plt\n\n# IMAGE_SIZE    = (299, 299)\n\n# test_image = image.load_img(\"../input/data/natural_images/motorbike/motorbike_0000.jpg\",target_size =IMAGE_SIZE )\n# test_image = image.img_to_array(test_image)\n# plt.imshow(test_image/255.)\n# test_image = np.expand_dims(test_image , axis = 0)\n\n# prediction = net_final.predict(test_image)\n# df = pd.DataFrame({'pred':prediction[0]})\n# print(prediction[0])\n# df.sort_values(by='pred', ascending=False, na_position='first')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}